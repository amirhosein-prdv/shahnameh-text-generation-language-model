{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4_zVQsxs-sFG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path = \"./dataset/shahnameh.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653849\n"
     ]
    }
   ],
   "source": [
    "f = open(path, 'rb')\n",
    "text = f.read().decode(encoding='utf-8')\n",
    "\n",
    "print(len(text)) # length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|به نام خداوند جان و خرد\n",
      "|کزین برتر اندیشه برنگذرد\n",
      "|خداوند نام و خداوند جای\n",
      "|خداوند روزی ده رهنمای\n",
      "|خداوند کیوان و گردان سپهر\n",
      "|فروزنده ماه و ناهید و مهر\n",
      "|ز نام و نشان و گمان برترست\n",
      "|نگارندهٔ بر شده پیکرست\n",
      "|به بینندگان آفریننده را\n",
      "|نبینی مرنجان دو بین\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " '|',\n",
       " '«',\n",
       " '»',\n",
       " '،',\n",
       " '؟',\n",
       " 'ء',\n",
       " 'آ',\n",
       " 'أ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ٔ',\n",
       " 'پ',\n",
       " 'چ',\n",
       " 'ژ',\n",
       " 'ک',\n",
       " 'گ',\n",
       " 'ی',\n",
       " '\\u200c']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '(': 2,\n",
       " ')': 3,\n",
       " '|': 4,\n",
       " '«': 5,\n",
       " '»': 6,\n",
       " '،': 7,\n",
       " '؟': 8,\n",
       " 'ء': 9,\n",
       " 'آ': 10,\n",
       " 'أ': 11,\n",
       " 'ؤ': 12,\n",
       " 'ئ': 13,\n",
       " 'ا': 14,\n",
       " 'ب': 15,\n",
       " 'ت': 16,\n",
       " 'ث': 17,\n",
       " 'ج': 18,\n",
       " 'ح': 19,\n",
       " 'خ': 20,\n",
       " 'د': 21,\n",
       " 'ذ': 22,\n",
       " 'ر': 23,\n",
       " 'ز': 24,\n",
       " 'س': 25,\n",
       " 'ش': 26,\n",
       " 'ص': 27,\n",
       " 'ض': 28,\n",
       " 'ط': 29,\n",
       " 'ظ': 30,\n",
       " 'ع': 31,\n",
       " 'غ': 32,\n",
       " 'ف': 33,\n",
       " 'ق': 34,\n",
       " 'ل': 35,\n",
       " 'م': 36,\n",
       " 'ن': 37,\n",
       " 'ه': 38,\n",
       " 'و': 39,\n",
       " 'ٔ': 40,\n",
       " 'پ': 41,\n",
       " 'چ': 42,\n",
       " 'ژ': 43,\n",
       " 'ک': 44,\n",
       " 'گ': 45,\n",
       " 'ی': 46,\n",
       " '\\u200c': 47}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2653849,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "l1VKcQHcymwb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'|به نام خداون' ---- characters mapped to int ---- > [ 4 15 38  1 37 14 36  1 20 21 14 39 37]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 = |\n",
      "15 = ب\n",
      "38 = ه\n",
      "1 =  \n",
      "37 = ن\n",
      "14 = ا\n",
      "36 = م\n",
      "1 =  \n",
      "20 = خ\n",
      "21 = د\n",
      "14 = ا\n",
      "39 = و\n",
      "37 = ن\n",
      "21 = د\n",
      "1 =  \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(15):\n",
    "    print(i.numpy(), end = ' = ')\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "l4hkDU3i7ozi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|خ'\n",
      "\n",
      "****\n",
      "****\n",
      "'داوند کیوان و گردان سپهر\\n|فروزنده ماه و ناهید و مهر\\n|ز نام و نشان و گمان برترست\\n|نگارندهٔ بر شده پیکر'\n",
      "\n",
      "****\n",
      "****\n",
      "'ست\\n|به بینندگان آفریننده را\\n|نبینی مرنجان دو بیننده را\\n|نیابد بدو نیز اندیشه راه\\n|که او برتر از نام و'\n",
      "\n",
      "****\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(3):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"\\n****\"*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "# For each sequence, duplicate and shift it to form the input and target text\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|'\n",
      "Target data: 'به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|خ'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 4 ('|')\n",
      "  expected output: 15 ('ب')\n",
      "Step    1\n",
      "  input: 15 ('ب')\n",
      "  expected output: 38 ('ه')\n",
      "Step    2\n",
      "  input: 38 ('ه')\n",
      "  expected output: 1 (' ')\n",
      "Step    3\n",
      "  input: 1 (' ')\n",
      "  expected output: 37 ('ن')\n",
      "Step    4\n",
      "  input: 37 ('ن')\n",
      "  expected output: 14 ('ا')\n",
      "Step    5\n",
      "  input: 14 ('ا')\n",
      "  expected output: 36 ('م')\n",
      "Step    6\n",
      "  input: 36 ('م')\n",
      "  expected output: 1 (' ')\n",
      "Step    7\n",
      "  input: 1 (' ')\n",
      "  expected output: 20 ('خ')\n",
      "Step    8\n",
      "  input: 20 ('خ')\n",
      "  expected output: 21 ('د')\n",
      "Step    9\n",
      "  input: 21 ('د')\n",
      "  expected output: 14 ('ا')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:10], target_example[:10])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 25\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(int(rnn_units/2),\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size=len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "C-_70kKAPrPU",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 48) #(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# try model\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model.predict(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"#(batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 25)            1200      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 512)           1101824   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (32, None, 1024)          6295552   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 48)            49200     \n",
      "=================================================================\n",
      "Total params: 7,447,776\n",
      "Trainable params: 7,447,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "# sample from the output distribution, to get actual character indices\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  7, 39, 28,  2, 22, 23, 35,  6, 31, 20,  7,  5, 37, 19,  2, 21,\n",
       "       12,  7, 11, 17, 38,  6, 11, 44, 18, 30,  4, 35, 47,  5,  6, 11, 19,\n",
       "       40,  4, 33, 42, 21, 14, 35, 47, 19, 19, 36, 31, 18, 29, 15, 15,  6,\n",
       "       33, 27, 24,  4, 23, 42, 23, 38,  1, 44,  0, 37,  1, 30, 21, 47, 45,\n",
       "       15, 24, 36, 32, 24, 21, 11,  9, 27, 42, 17, 24, 28, 12,  6, 12, 32,\n",
       "       45, 16,  9, 47, 31, 12, 45, 46, 15, 40,  7,  1, 13, 25, 31],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|'\n",
      "\n",
      "Next Char Predictions: \n",
      " '\\n،وض(ذرل»عخ،«نح(دؤ،أثه»أکجظ|ل\\u200c«»أحٔ|فچدال\\u200cححمعجطبب»فصز|رچره ک\\nن ظد\\u200cگبزمغزدأءصچثزضؤ»ؤغگتء\\u200cعؤگیبٔ، ئسع'\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "821/821 [==============================] - 56s 67ms/step - loss: 2.1872 - accuracy: 0.3713\n",
      "Epoch 2/15\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 1.4672 - accuracy: 0.5621\n",
      "Epoch 3/15\n",
      "821/821 [==============================] - 56s 69ms/step - loss: 1.3327 - accuracy: 0.5974\n",
      "Epoch 4/15\n",
      "821/821 [==============================] - 57s 69ms/step - loss: 1.2634 - accuracy: 0.6165\n",
      "Epoch 5/15\n",
      "821/821 [==============================] - 57s 70ms/step - loss: 1.2105 - accuracy: 0.6311\n",
      "Epoch 6/15\n",
      "821/821 [==============================] - 57s 70ms/step - loss: 1.1636 - accuracy: 0.6442\n",
      "Epoch 7/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 1.1204 - accuracy: 0.6564\n",
      "Epoch 8/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 1.0809 - accuracy: 0.6678\n",
      "Epoch 9/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 1.0444 - accuracy: 0.6781\n",
      "Epoch 10/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 1.0115 - accuracy: 0.6877\n",
      "Epoch 11/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 0.9785 - accuracy: 0.6971\n",
      "Epoch 12/15\n",
      "821/821 [==============================] - 58s 71ms/step - loss: 0.9483 - accuracy: 0.7058\n",
      "Epoch 13/15\n",
      "821/821 [==============================] - 58s 70ms/step - loss: 0.9211 - accuracy: 0.7135\n",
      "Epoch 14/15\n",
      "821/821 [==============================] - 58s 70ms/step - loss: 0.8936 - accuracy: 0.7214\n",
      "Epoch 15/15\n",
      "821/821 [==============================] - 57s 70ms/step - loss: 0.8679 - accuracy: 0.7290\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 1000\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "گهی با رستم شیردل رستخیز\n",
      "|به افراسیاب آن خوره نوشد چه گریان زرین نهید\n",
      "|پراز برف چاچین و با دانیم\n",
      "|یکی تیر زد بر کران تاج وتخت\n",
      "|شما را ز عنبر بسی سرخ و زشت\n",
      "|کجا آن بزرگان چو خرم بهار\n",
      "|گهرها ز گیتی مبندی بود\n",
      "|چنویتر\n",
      "|ز پروردهٔ شاه نوشین روان\n",
      "|چو هشتادسان بوده باشد نگاه\n",
      "|به پیش پدر سرخ گردنده هست\n",
      "|کجا توشهٔ شهریاری بود\n",
      "|بدو ماه و خورشید و فریادرس\n",
      "|مهندست مردی ز بیم گزند\n",
      "|که مه سالخورشخونشی و بانگ بود\n",
      "|بدین آرزو رای و فرمان تو راست\n",
      "|بود روز دستان پس شهریار\n",
      "|همان پیشکابا برو گوی تهر\n",
      "|ع\n",
      "|همی برتو خواهی که او را گزید\n",
      "|بناگاه شد سوی آن رزمگاه\n",
      "|شد آن کشته آن کوه و هم دشمنست\n",
      "|کزو یادگیری همان سرزنش\n",
      "|به کاری نجستست با من به جنگ\n",
      "|جهان بی‌هنر وسپی خیره گشت\n",
      "|که کار آمدمانان خورش\n",
      "|ز کافور مغز تو را کن بساخت\n",
      "|ز دیهیم شاهی و تخت و کلاه\n",
      "|کس اندر جهان این شود گوشتند\n",
      "|ببست و بیاراست و آمد به کار\n",
      "|ششم تیره گر زند و استا مگوی\n",
      "|چنین داد پاسخ ورا کی قرا\n",
      "|بیارد کس او را نشاید نشان\n",
      "|گه آمد کنی خویشتن در جهان\n",
      "|ز بالا ببارید بر چارباد\n",
      "|گر از پوشش از بوی و رگشان رسید\n",
      "|بجستند تازان به بانگ خروس\n",
      "|ز بدخواه برخواندم\n",
      "|رسیده به در\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"گهی با رستم\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_shahnameh-text-generation-language-model.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
