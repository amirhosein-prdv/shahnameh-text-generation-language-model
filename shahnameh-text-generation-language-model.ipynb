{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovpZyIhNIgoq"
   },
   "source": [
    "# Text generation with an RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGyKZj3bzf9p"
   },
   "source": [
    "### libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4_zVQsxs-sFG"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "pD_55cOxLkAb"
   },
   "outputs": [],
   "source": [
    "path = \"./dataset/shahnameh.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UHjdCjDuSvX_"
   },
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "aavnuByVymwK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2653849\n"
     ]
    }
   ],
   "source": [
    "f = open(path, 'rb')\n",
    "text = f.read().decode(encoding='utf-8')\n",
    "\n",
    "print(len(text)) # length of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Duhg9NrUymwO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|به نام خداوند جان و خرد\n",
      "|کزین برتر اندیشه برنگذرد\n",
      "|خداوند نام و خداوند جای\n",
      "|خداوند روزی ده رهنمای\n",
      "|خداوند کیوان و گردان سپهر\n",
      "|فروزنده ماه و ناهید و مهر\n",
      "|ز نام و نشان و گمان برترست\n",
      "|نگارندهٔ بر شده پیکرست\n",
      "|به بینندگان آفریننده را\n",
      "|نبینی مرنجان دو بین\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "IlCgQBRVymwR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 unique characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '(',\n",
       " ')',\n",
       " '|',\n",
       " '«',\n",
       " '»',\n",
       " '،',\n",
       " '؟',\n",
       " 'ء',\n",
       " 'آ',\n",
       " 'أ',\n",
       " 'ؤ',\n",
       " 'ئ',\n",
       " 'ا',\n",
       " 'ب',\n",
       " 'ت',\n",
       " 'ث',\n",
       " 'ج',\n",
       " 'ح',\n",
       " 'خ',\n",
       " 'د',\n",
       " 'ذ',\n",
       " 'ر',\n",
       " 'ز',\n",
       " 'س',\n",
       " 'ش',\n",
       " 'ص',\n",
       " 'ض',\n",
       " 'ط',\n",
       " 'ظ',\n",
       " 'ع',\n",
       " 'غ',\n",
       " 'ف',\n",
       " 'ق',\n",
       " 'ل',\n",
       " 'م',\n",
       " 'ن',\n",
       " 'ه',\n",
       " 'و',\n",
       " 'ٔ',\n",
       " 'پ',\n",
       " 'چ',\n",
       " 'ژ',\n",
       " 'ک',\n",
       " 'گ',\n",
       " 'ی',\n",
       " '\\u200c']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print('{} unique characters'.format(len(vocab)))\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNnrKn_lL-IJ"
   },
   "source": [
    "## Process the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFjSVAlWzf-N"
   },
   "source": [
    "### Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IalZLbvOzf-F"
   },
   "outputs": [],
   "source": [
    "# Creating a mapping from unique characters to indices\n",
    "char2idx = {u:i for i, u in enumerate(vocab)}\n",
    "idx2char = np.array(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '(': 2,\n",
       " ')': 3,\n",
       " '|': 4,\n",
       " '«': 5,\n",
       " '»': 6,\n",
       " '،': 7,\n",
       " '؟': 8,\n",
       " 'ء': 9,\n",
       " 'آ': 10,\n",
       " 'أ': 11,\n",
       " 'ؤ': 12,\n",
       " 'ئ': 13,\n",
       " 'ا': 14,\n",
       " 'ب': 15,\n",
       " 'ت': 16,\n",
       " 'ث': 17,\n",
       " 'ج': 18,\n",
       " 'ح': 19,\n",
       " 'خ': 20,\n",
       " 'د': 21,\n",
       " 'ذ': 22,\n",
       " 'ر': 23,\n",
       " 'ز': 24,\n",
       " 'س': 25,\n",
       " 'ش': 26,\n",
       " 'ص': 27,\n",
       " 'ض': 28,\n",
       " 'ط': 29,\n",
       " 'ظ': 30,\n",
       " 'ع': 31,\n",
       " 'غ': 32,\n",
       " 'ف': 33,\n",
       " 'ق': 34,\n",
       " 'ل': 35,\n",
       " 'م': 36,\n",
       " 'ن': 37,\n",
       " 'ه': 38,\n",
       " 'و': 39,\n",
       " 'ٔ': 40,\n",
       " 'پ': 41,\n",
       " 'چ': 42,\n",
       " 'ژ': 43,\n",
       " 'ک': 44,\n",
       " 'گ': 45,\n",
       " 'ی': 46,\n",
       " '\\u200c': 47}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2653849,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_as_int = np.array([char2idx[c] for c in text])\n",
    "text_as_int.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "l1VKcQHcymwb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'|به نام خداون' ---- characters mapped to int ---- > [ 4 15 38  1 37 14 36  1 20 21 14 39 37]\n"
     ]
    }
   ],
   "source": [
    "# Show how the first 13 characters from the text are mapped to integers\n",
    "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hgsVvVxnymwf"
   },
   "source": [
    "### Create training examples and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "0UHJDA39zf-O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 = |\n",
      "15 = ب\n",
      "38 = ه\n",
      "1 =  \n",
      "37 = ن\n",
      "14 = ا\n",
      "36 = م\n",
      "1 =  \n",
      "20 = خ\n",
      "21 = د\n",
      "14 = ا\n",
      "39 = و\n",
      "37 = ن\n",
      "21 = د\n",
      "1 =  \n"
     ]
    }
   ],
   "source": [
    "# The maximum length sentence\n",
    "seq_length = 100\n",
    "\n",
    "# Create training examples\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
    "\n",
    "for i in char_dataset.take(15):\n",
    "    print(i.numpy(), end = ' = ')\n",
    "    print(idx2char[i.numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "l4hkDU3i7ozi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|خ'\n",
      "\n",
      "****\n",
      "****\n",
      "'داوند کیوان و گردان سپهر\\n|فروزنده ماه و ناهید و مهر\\n|ز نام و نشان و گمان برترست\\n|نگارندهٔ بر شده پیکر'\n",
      "\n",
      "****\n",
      "****\n",
      "'ست\\n|به بینندگان آفریننده را\\n|نبینی مرنجان دو بیننده را\\n|نیابد بدو نیز اندیشه راه\\n|که او برتر از نام و'\n",
      "\n",
      "****\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for item in sequences.take(3):\n",
    "    print(repr(''.join(idx2char[item.numpy()])))\n",
    "    print(\"\\n****\"*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "9NGu-FkO_kYU"
   },
   "outputs": [],
   "source": [
    "# For each sequence, duplicate and shift it to form the input and target text\n",
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "GNbw-iR0ymwj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  '|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|'\n",
      "Target data: 'به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|خ'\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "    print ('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n",
    "    print ('Target data:', repr(''.join(idx2char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0eBu9WZG84i0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 4 ('|')\n",
      "  expected output: 15 ('ب')\n",
      "Step    1\n",
      "  input: 15 ('ب')\n",
      "  expected output: 38 ('ه')\n",
      "Step    2\n",
      "  input: 38 ('ه')\n",
      "  expected output: 1 (' ')\n",
      "Step    3\n",
      "  input: 1 (' ')\n",
      "  expected output: 37 ('ن')\n",
      "Step    4\n",
      "  input: 37 ('ن')\n",
      "  expected output: 14 ('ا')\n",
      "Step    5\n",
      "  input: 14 ('ا')\n",
      "  expected output: 36 ('م')\n",
      "Step    6\n",
      "  input: 36 ('م')\n",
      "  expected output: 1 (' ')\n",
      "Step    7\n",
      "  input: 1 (' ')\n",
      "  expected output: 20 ('خ')\n",
      "Step    8\n",
      "  input: 20 ('خ')\n",
      "  expected output: 21 ('د')\n",
      "Step    9\n",
      "  input: 21 ('د')\n",
      "  expected output: 14 ('ا')\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:10], target_example[:10])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
    "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MJdfPmdqzf-R"
   },
   "source": [
    "### Create training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "p2pGotuNzf-S"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((32, 100), (32, 100)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6oUuElIMgVx"
   },
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "zHT8cLh7EAsg"
   },
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "# The embedding dimension\n",
    "embedding_dim = 25\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "MtCrdfzEI2N0"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(int(rnn_units/2),\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.LSTM(rnn_units,\n",
    "                    return_sequences=True,\n",
    "                    stateful=True,\n",
    "                    recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "wwsrpOik5zhv"
   },
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size=len(vocab),\n",
    "  embedding_dim=embedding_dim,\n",
    "  rnn_units=rnn_units,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "C-_70kKAPrPU",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 48) #(batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# try model\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model.predict(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"#(batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vPGmAAXmVLGC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (32, None, 25)            1200      \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (32, None, 512)           1101824   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (32, None, 1024)          6295552   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, None, 48)            49200     \n",
      "=================================================================\n",
      "Total params: 7,447,776\n",
      "Trainable params: 7,447,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "4V4MfFg0RQJg"
   },
   "outputs": [],
   "source": [
    "# sample from the output distribution, to get actual character indices\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "YqFMUQc_UFgM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  7,  2, 36, 24, 34, 38, 18, 23, 12, 36,  2, 42,  0,  3, 15, 11,\n",
       "        3,  4, 27, 30, 36, 24, 26, 34,  7,  6, 31, 42, 13, 29, 25, 29, 36,\n",
       "       44,  7, 24, 34, 24, 32,  6,  1, 32, 14, 10,  0,  1, 27,  2, 34,  0,\n",
       "       10, 21,  7,  8, 38, 32, 29, 36,  7, 26, 19,  9,  5,  2, 17, 28, 44,\n",
       "       38, 11, 13, 25, 47, 46, 32,  6,  8, 26, 38, 40, 15,  8, 39, 15, 43,\n",
       "        2, 23, 45,  3, 25, 41, 22,  4, 44,  8, 14, 19, 13, 31, 23],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "xWcFwPwLSo05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: \n",
      " '|به نام خداوند جان و خرد\\n|کزین برتر اندیشه برنگذرد\\n|خداوند نام و خداوند جای\\n|خداوند روزی ده رهنمای\\n|'\n",
      "\n",
      "Next Char Predictions: \n",
      " '«،(مزقهجرؤم(چ\\n)بأ)|صظمزشق،»عچئطسطمک،زقزغ» غاآ\\n ص(ق\\nآد،؟هغطم،شحء«(ثضکهأئس\\u200cیغ»؟شهٔب؟وبژ(رگ)سپذ|ک؟احئعر'\n"
     ]
    }
   ],
   "source": [
    "# Decode these to see the text predicted\n",
    "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
    "print()\n",
    "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJL0Q0YPY6Ee"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "4HrXTACTdzY-"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "DDl1_Een6rL0"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ieSJdchZggUj"
   },
   "source": [
    "### Configure checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "W6fWTriUZP-n"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Ky3F_BhgkTW"
   },
   "source": [
    "### Execute the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "UK-hmKjYVoll"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "821/821 [==============================] - 55s 63ms/step - loss: 2.0836 - accuracy: 0.3982\n",
      "Epoch 2/30\n",
      "821/821 [==============================] - 53s 64ms/step - loss: 1.4111 - accuracy: 0.5772\n",
      "Epoch 3/30\n",
      "821/821 [==============================] - 52s 63ms/step - loss: 1.2942 - accuracy: 0.6080\n",
      "Epoch 4/30\n",
      "821/821 [==============================] - 53s 64ms/step - loss: 1.2252 - accuracy: 0.6273\n",
      "Epoch 5/30\n",
      "821/821 [==============================] - 54s 65ms/step - loss: 1.1681 - accuracy: 0.6433\n",
      "Epoch 6/30\n",
      "821/821 [==============================] - 54s 66ms/step - loss: 1.1159 - accuracy: 0.6585\n",
      "Epoch 7/30\n",
      "821/821 [==============================] - 54s 66ms/step - loss: 1.0678 - accuracy: 0.6723\n",
      "Epoch 8/30\n",
      "821/821 [==============================] - 54s 66ms/step - loss: 1.0233 - accuracy: 0.6851\n",
      "Epoch 9/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.9828 - accuracy: 0.6967\n",
      "Epoch 10/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.9443 - accuracy: 0.7082\n",
      "Epoch 11/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.9094 - accuracy: 0.7183\n",
      "Epoch 12/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.8756 - accuracy: 0.7282\n",
      "Epoch 13/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.8458 - accuracy: 0.73680s - loss: 0.8457 - accuracy\n",
      "Epoch 14/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.8204 - accuracy: 0.7439\n",
      "Epoch 15/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.7981 - accuracy: 0.7503\n",
      "Epoch 16/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.7748 - accuracy: 0.75690s - loss: 0.7748 - accuracy: 0.75\n",
      "Epoch 17/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.7550 - accuracy: 0.7625\n",
      "Epoch 18/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.7361 - accuracy: 0.7677\n",
      "Epoch 19/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.7167 - accuracy: 0.7734\n",
      "Epoch 20/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6978 - accuracy: 0.7788\n",
      "Epoch 21/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6835 - accuracy: 0.7830\n",
      "Epoch 22/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.6711 - accuracy: 0.7862\n",
      "Epoch 23/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6580 - accuracy: 0.7902\n",
      "Epoch 24/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.6467 - accuracy: 0.7930\n",
      "Epoch 25/30\n",
      "821/821 [==============================] - 57s 70ms/step - loss: 0.6379 - accuracy: 0.7952\n",
      "Epoch 26/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6268 - accuracy: 0.7985\n",
      "Epoch 27/30\n",
      "821/821 [==============================] - 56s 68ms/step - loss: 0.6196 - accuracy: 0.8003\n",
      "Epoch 28/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6127 - accuracy: 0.8022\n",
      "Epoch 29/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6046 - accuracy: 0.8044\n",
      "Epoch 30/30\n",
      "821/821 [==============================] - 55s 67ms/step - loss: 0.6001 - accuracy: 0.8056\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKkD5M6eoSiN"
   },
   "source": [
    "## Generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "LycQ-ot_jjyu"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjGz1tDkzf-u"
   },
   "source": [
    "### The prediction loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "WvuwZBX5Ogfd"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    # Evaluation step (generating text using the learned model)\n",
    "\n",
    "    # Number of characters to generate\n",
    "    num_generate = 100\n",
    "\n",
    "    # Converting our start string to numbers (vectorizing)\n",
    "    input_eval = [char2idx[s] for s in start_string]\n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    # Empty string to store our results\n",
    "    text_generated = []\n",
    "\n",
    "\n",
    "    # Here batch size == 1\n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        # remove the batch dimension\n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        # using a categorical distribution to predict the word returned by the model\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        # We pass the predicted word as the next input to the model\n",
    "        # along with the previous hidden state\n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(idx2char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ktovv0RFhrkn"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "به نام خداوند بخت\n",
      "|که او را به نزد کسان پر ز غم\n",
      "|برون آمد از بیشه و پتنگ\n",
      "|زنی تا سوی مهتر اندر دراز\n",
      "|همی کشته شد \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_string=u\"به نام خداوند\"))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "08_shahnameh-text-generation-language-model.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
